{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLCT Estimation of Sorting\n",
    "\n",
    "This Jupyter Notebook aims to measure the Real Log Canonical Threshold (RLCT) for a small 3-layer transformer model (~280,000 parameters) trained to sort sequences of 20 digits consisting of the numbers 0-19. It uses both Stochastic Gradient Nose-Hoover Thermostat (SGNHT) and Stochastic Gradient Langevin Dynamics (SGLD) as sampling methods.\n",
    "\n",
    "## Main Steps:\n",
    "\n",
    "1. **Data Preparation**: Generate the dataset of numbers to sort.\n",
    "2. **Model Training**: Train a transformer model using stochastic gradient descent.\n",
    "3. **Model Evaluation**: Evaluate the model's performance on a test set.\n",
    "4. **RLCT Estimation**: Use SGNHT and SGLD samplers to estimate RLCT.\n",
    "5. **Plotting**: Visualize train and test losses, and RLCT estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install devinterp seaborn torchvision pickleshare transformer_lens pytest\n",
    "!git clone https://github.com/ucla-vision/entropy-sgd.git\n",
    "%cd entropy-sgd\n",
    "from python.optim import EntropySGD  \n",
    "%cd ..\n",
    "!git clone https://github.com/deepmind/tracr\n",
    "%cd tracr\n",
    "%pip install .\n",
    "%cd ..\n",
    "!git clone https://github.com/shaffreybenjamin/sortinterp.git\n",
    "%cd sortinterp\n",
    "%pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_default_matmul_precision', 'float32')\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler import lib\n",
    "from tracr.rasp import rasp\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "\n",
    "from sortinterp.utils import cfg_from_tracr, load_tracr_weights\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from python.optim import EntropySGD\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.optim.sgnht import SGNHT\n",
    "\n",
    "PRIMARY, SECONDARY, TERTIARY, QUATERNARY = sns.color_palette(\"muted\")[:4]\n",
    "PRIMARY_LIGHT, SECONDARY_LIGHT, TERTIARY_LIGHT, QUATERNARY_LIGHT = sns.color_palette(\n",
    "    \"pastel\"\n",
    ")[:4]\n",
    "\n",
    "input_size = 20 # Length of sequences\n",
    "vocab_size = 20 # Vocabulary size\n",
    "\n",
    "vocab = {*range(vocab_size)}\n",
    "program = lib.make_sort(rasp.tokens, rasp.tokens, max_seq_len=input_size, min_key=0)\n",
    "\n",
    "tracr_model = compiling.compile_rasp_to_model(\n",
    "    program=program,\n",
    "    vocab=vocab,\n",
    "    max_seq_len=input_size,\n",
    "    compiler_bos=\"bos\",\n",
    "    mlp_exactness=100)\n",
    "\n",
    "cfg = cfg_from_tracr(tracr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_rows(tensor_a, tensor_b):\n",
    "    \"\"\"\n",
    "    Remove rows from tensor_a that appear in tensor_b.\n",
    "\n",
    "    :param tensor_a: The original tensor from which to remove rows\n",
    "    :param tensor_b: The tensor containing rows to be removed from tensor_a\n",
    "    :return: A tensor with the rows removed\n",
    "    \"\"\"\n",
    "    # Expand tensor_a and tensor_b to be able to compare each row\n",
    "    expanded_a = tensor_a.unsqueeze(1)  # Shape: (num_rows_a, 1, num_columns)\n",
    "    expanded_b = tensor_b.unsqueeze(0)  # Shape: (1, num_rows_b, num_columns)\n",
    "\n",
    "    # Compare each row of tensor_a with each row of tensor_b\n",
    "    comparison = (expanded_a == expanded_b).all(dim=2)  # Shape: (num_rows_a, num_rows_b)\n",
    "\n",
    "    # Create a mask for rows in tensor_a that do not match any row in tensor_b\n",
    "    mask = ~comparison.any(dim=1)  # Shape: (num_rows_a)\n",
    "\n",
    "    # Filter tensor_a using the mask\n",
    "    filtered_tensor = tensor_a[mask]\n",
    "\n",
    "    return filtered_tensor\n",
    "\n",
    "# this dataset is intended as a control\n",
    "def get_dataset_1(input_size, vocab_size):\n",
    "    # generate random sequences of size input_size using numbers between 0 and vocab_size\n",
    "    sequences = torch.randint(0, vocab_size, (4 * (input_size * (vocab_size - 2) + 1), input_size))\n",
    "    \n",
    "    # split into train and test\n",
    "    split = int(0.333 * len(sequences))\n",
    "    train_sequences = sequences[ : split]\n",
    "    test_sequences = sequences[split : ]\n",
    "    return train_sequences, test_sequences\n",
    "\n",
    "# this dataset is intended to incentivise the model to learn a simpler algorithm than sorting, \n",
    "# namely putting the nonzero number at the end\n",
    "def get_dataset_2(input_size, vocab_size):\n",
    "    # construct all sequences that consist entirely of zeros except for one non-zero element \n",
    "    # which will be a number between 1 and vocab_size - 1\n",
    "    sequences = torch.eye(input_size).unsqueeze(dim=0) * torch.arange(1, vocab_size - 1).reshape(-1, 1, 1)\n",
    "    # include all zeros sequence\n",
    "    train_sequences = torch.cat((torch.zeros(1, sequences.size(dim=1)), sequences.reshape(-1, input_size)), dim=0).long()\n",
    "    \n",
    "    # test sequences are sequences containing any of the digits from 0 to vocab_size\n",
    "    test_sequences = torch.randint(0, vocab_size, (3 * (input_size * (vocab_size - 2) + 1), input_size))\n",
    "    \n",
    "    # ensure that we remove possible training elements\n",
    "    test_sequences = remove_common_rows(test_sequences, train_sequences)\n",
    "    \n",
    "    # include a small amount of the `correct' signal in the training data\n",
    "    # so that the model can still potentially learn the correct algorithm\n",
    "    split = int(0.333 * len(train_sequences))\n",
    "    train_sequences = torch.cat((train_sequences, test_sequences[ : split]), dim=0)\n",
    "    test_sequences = test_sequences[split : ]\n",
    "    return train_sequences, test_sequences\n",
    "\n",
    "# this dataset is intendent to incentivise the model to learn a sorting algorithm specific to certain digits only\n",
    "def get_dataset_3(input_size, vocab_size):\n",
    "    # ensure that training sequences consist of primarily of sequences containing numbers from 0 to middle\n",
    "    middle = vocab_size // 2\n",
    "    train_sequences = torch.randint(0, middle, ((input_size * (vocab_size - 2) + 1), input_size))\n",
    "    \n",
    "    # test sequences consist of sequences containing numbers from middle to vocab_size -1\n",
    "    test_sequences = torch.randint(middle, vocab_size, (3 * (input_size * (vocab_size - 2) + 1), input_size))\n",
    "    \n",
    "    # include a small amount of the `correct' signal in the training data\n",
    "    # so that the model can still potentially learn the correct algorithm\n",
    "    split = int(0.333 * len(train_sequences))\n",
    "    train_sequences = torch.cat((train_sequences, test_sequences[ : split]), dim=0)\n",
    "    test_sequences = test_sequences[split : ]\n",
    "    return train_sequences, test_sequences\n",
    "    \n",
    "\n",
    "def get_data(input_size, vocab_size, dataset=1):\n",
    "    if dataset == 0:\n",
    "        train_sequences, test_sequences = get_dataset_1(input_size, vocab_size)\n",
    "    elif dataset == 1:\n",
    "        train_sequences, test_sequences = get_dataset_2(input_size, vocab_size)\n",
    "    elif dataset == 2:\n",
    "        train_sequences, test_sequences = get_dataset_3(input_size, vocab_size)\n",
    "    else:\n",
    "        print('enter a dataset number between 0 and 2')\n",
    "        \n",
    "    train_sequences_sorted = torch.sort(train_sequences, dim=1).values\n",
    "    test_sequences_sorted = torch.sort(test_sequences, dim=1).values\n",
    "    train_data = list(zip(train_sequences, train_sequences_sorted))\n",
    "    test_data = list(zip(test_sequences,  test_sequences_sorted))\n",
    "    return train_data, test_data\n",
    "\n",
    "dataset = 2\n",
    "train_data, test_data = get_data(input_size, vocab_size, dataset)\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Test size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(outputs, targets):\n",
    "    return (outputs.argmax(1) == targets).float().mean()\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, model_key):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for index, (data, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.to(DEVICE))\n",
    "        outputs = outputs.permute(0, 2, 1)\n",
    "        loss = criterion(outputs, targets.to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if index == 0:\n",
    "            accuracy = accuracy_function(outputs, targets.to(DEVICE))\n",
    "            print(f'batch {index}, loss: {loss.item()}', f'accuracy: {accuracy.item()}')\n",
    "        \n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for index, (data, targets) in enumerate(test_loader):\n",
    "            #outputs, cache = model.run_with_cache(data.to(DEVICE))\n",
    "            outputs = model(data.to(DEVICE))\n",
    "            outputs = outputs.permute(0, 2, 1)\n",
    "            loss = criterion(outputs, targets.to(DEVICE))\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    return test_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 512\n",
    "LR = 0.01\n",
    "N_EPOCHS = 200\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_loader, test_loader, criterion, runs):\n",
    "    train_losses = torch.zeros(runs, N_EPOCHS)\n",
    "    test_losses = torch.zeros(runs, N_EPOCHS)\n",
    "    models_saved = []\n",
    "    for run in tqdm(range(runs)):\n",
    "        model = HookedTransformer(cfg)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        for epoch in tqdm(range(N_EPOCHS)):\n",
    "            train_loss = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion, 'sgd'\n",
    "            )\n",
    "            test_loss = evaluate(model, test_loader, criterion)\n",
    "            train_losses[run, epoch] = train_loss\n",
    "            test_losses[run, epoch] = test_loss\n",
    "            models_saved += [copy.deepcopy(model)]\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}, Model {'sgd'.upper()} Train Loss: {train_loss}, Test Loss: {test_loss}\"\n",
    "            )\n",
    "        \n",
    "    train_losses_final = train_losses.mean(dim=0)\n",
    "    test_losses_final = test_losses.mean(dim=0)\n",
    "    return train_losses_final, test_losses_final, models_saved\n",
    "\n",
    "runs = 5\n",
    "train_losses_final, test_losses_final, models_saved = train_models(train_loader, test_loader, criterion, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from devinterp.slt import estimate_learning_coeff_with_summary\n",
    "\n",
    "\n",
    "def estimate_rlcts(models, train_loader, criterion, data_length, device, num_draws):\n",
    "    estimates = {\"sgnht\": [], \"sgld\": []}\n",
    "    for idx, model in enumerate(tqdm(models)):\n",
    "        for method, optimizer_kwargs in [\n",
    "            (\"sgnht\", {\"lr\": 1e-7, \"diffusion_factor\": 0.01}),\n",
    "            (\"sgld\", {\"lr\": 1e-5, \"localization\": 100.0}),\n",
    "        ]:\n",
    "            results = estimate_learning_coeff_with_summary(\n",
    "                model,\n",
    "                train_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer_kwargs=optimizer_kwargs,\n",
    "                sampling_method=SGNHT if method == \"sgnht\" else SGLD,\n",
    "                num_chains=1,\n",
    "                num_draws=num_draws,\n",
    "                num_burnin_steps=0,\n",
    "                num_steps_bw_draws=1,\n",
    "                device=device,\n",
    "            )\n",
    "            estimate = results[\"llc/mean\"]\n",
    "            \n",
    "            # take losses from last chain for plotting\n",
    "            if idx == N_EPOCHS - 1:\n",
    "                losses = results['loss/trace']\n",
    "            estimates[method].append(estimate)\n",
    "    return estimates, losses\n",
    "\n",
    "def obtain_rlct_estimates(train_loader, models_saved, criterion, runs):\n",
    "    data_length = len(train_loader.dataset)\n",
    "    rlct_estimates = {\"sgnht\": torch.zeros(runs, N_EPOCHS), \"sgld\": torch.zeros(runs, N_EPOCHS)}\n",
    "    num_draws = 400\n",
    "    last_chain_losses = torch.zeros(runs, num_draws)\n",
    "\n",
    "    for run in tqdm(range(runs)):\n",
    "        rlct_estimate, losses = estimate_rlcts(\n",
    "            models_saved[N_EPOCHS * run : N_EPOCHS * (run + 1)], train_loader, criterion, data_length, DEVICE, num_draws\n",
    "        )\n",
    "        #rlct_estimates[\"sgnht\"][run] = torch.tensor(rlct_estimate[\"sgnht\"])\n",
    "        rlct_estimates[\"sgld\"][run] = torch.tensor(rlct_estimate[\"sgld\"])\n",
    "        last_chain_losses[run] = torch.tensor(losses)\n",
    "\n",
    "    rlct_estimates_final = {\"sgnht\": rlct_estimates[\"sgnht\"].mean(dim=0), \"sgld\": rlct_estimates[\"sgld\"].mean(dim=0)}\n",
    "    return rlct_estimates_final, last_chain_losses.mean(dim=0)\n",
    "\n",
    "rlct_estimates_final, last_chain_losses_final = obtain_rlct_estimates(train_loader, models_saved, criterion, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses_final, test_losses_final, dataset):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=PRIMARY)\n",
    "    ax1.plot(train_losses_final, label=\"Train Loss, sgd\", color=PRIMARY)\n",
    "    ax1.plot(test_losses_final, label=\"Test Loss, sgd\", color=PRIMARY_LIGHT)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"losses_\" + str(dataset) + \"_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "\n",
    "def plot_rclts(rlct_estimates_final, dataset):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    fig, ax2 = plt.subplots(figsize=(10, 6))\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"Local Learning Coefficient, $\\hat \\lambda$\", color=SECONDARY)\n",
    "    #ax2.plot(rlct_estimates_final[\"sgnht\"], label=\"SGNHT, sgd\", color=TERTIARY)\n",
    "    ax2.plot(rlct_estimates_final[\"sgld\"], label=\"SGLD, sgd\", color=TERTIARY_LIGHT)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=SECONDARY)\n",
    "    ax2.legend(loc=\"center right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"rclt_\" + str(dataset) + \"_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "\n",
    "def plot_losses_chain(last_chain_losses_final, dataset):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel(\"Draw\")\n",
    "    ax1.set_ylabel(\"Loss\", color=PRIMARY)\n",
    "    ax1.plot(last_chain_losses_final, label=\"Loss, sgd\", color=PRIMARY)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"last_chain_losses_\" + str(dataset) + \"_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "\n",
    "plot_losses(train_losses_final, test_losses_final, dataset)\n",
    "plot_rclts(rlct_estimates_final, dataset)\n",
    "plot_losses_chain(last_chain_losses_final, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiments(dataset=1):\n",
    "    train_data, test_data = get_data(input_size, vocab_size, dataset)\n",
    "    train_size = len(train_data)\n",
    "    test_size = len(test_data)\n",
    "    print(f\"Train size: {train_size}\")\n",
    "    print(f\"Test size: {test_size}\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    runs = 5\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    train_losses_final, test_losses_final, models_saved = train_models(train_loader, test_loader, criterion, runs)\n",
    "    rlct_estimates_final = obtain_rlct_estimates(train_loader, models_saved, criterion, runs)\n",
    "    \n",
    "    plot_losses(train_losses_final, test_losses_final, dataset)\n",
    "    plot_rclts(rlct_estimates_final, dataset)\n",
    "\n",
    "for num in range(3):\n",
    "    run_experiments(dataset=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
